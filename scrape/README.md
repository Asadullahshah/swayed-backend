# AI Content Suggestor API

A FastAPI-based service that orchestrates the complete social media content scraping and selection pipeline.

## ğŸš€ Features

- **Multi-platform support**: Instagram, LinkedIn, Twitter/X, YouTube, TikTok
- **Complete pipeline**: Scraping â†’ Processing â†’ Content Selection
- **Intelligent scoring**: Platform-specific engagement algorithms
- **Even distribution**: Content balanced across URL groups
- **Post numbering**: Sequential numbering (post_1, post_2, etc.)
- **Async processing**: Background tasks for long-running operations

## ğŸ“‹ API Endpoints

### `POST /process-content`

Submit URLs for complete pipeline processing.

**Request Body:**
```json
{
  "urls": [
    "https://www.instagram.com/natgeo/",
    "https://www.linkedin.com/in/andrewyng/",
    "https://x.com/theinsiderzclub",
    "https://www.youtube.com/@NatGeo",
    "https://www.tiktok.com/@natgeo"
  ]
}
```

**Response:**
```json
{
  "task_id": "task_20231203_143022_123456",
  "status": "started",
  "message": "Started processing 5 URLs",
  "urls_detected": [...],
  "platforms_needed": ["instagram", "linkedin", "twitter", "youtube", "tiktok"]
}
```

### `GET /results/{task_id}`

Get processing results for a specific task.

**Response:**
```json
{
  "task_id": "task_20231203_143022_123456",
  "status": "completed",
  "result_data": [
    {
      "post_number": "post_1",
      "platform": "twitter",
      "content_type": "tweet",
      "type": "text",
      "text": "Amazing content...",
      "stats": {
        "views": 15420,
        "likes": 892,
        "retweets": 156,
        "replies": 43
      },
      "author": {
        "name": "The Insiders Club",
        "username": "theinsiderzclub",
        "profile_url": "https://x.com/theinsiderzclub"
      },
      "engagement_score": 1245.67,
      "URL_GROUP": "https://x.com/theinsiderzclub"
    }
  ]
}
```

### `GET /health`

Health check endpoint.

## ğŸ› ï¸ Setup and Usage

### 1. Install Dependencies

```bash
pip install -r requirements.txt
```

### 2. Configure Environment

Make sure your `.env` file contains:
```
APIFY_KEY=your_apify_api_key
OPENAI_KEY=your_openai_api_key
```

### 3. Start the Server

```bash
# Navigate to the scrape directory
cd scrape

# Start the FastAPI server
python main.py
```

The API will be available at `http://localhost:8000`

### 4. Test the API

#### Using the Test Script
```bash
python test_api.py
```

#### Using curl
```bash
# Submit URLs for processing
curl -X POST "http://localhost:8000/process-content" \
     -H "Content-Type: application/json" \
     -d '{
       "urls": [
         "https://www.instagram.com/natgeo/",
         "https://x.com/theinsiderzclub"
       ]
     }'

# Check results (replace with actual task_id)
curl "http://localhost:8000/results/task_20231203_143022_123456"
```

#### Using Python requests
```python
import requests
import time

# Submit URLs
response = requests.post("http://localhost:8000/process-content", json={
    "urls": [
        "https://www.instagram.com/natgeo/",
        "https://x.com/theinsiderzclub"
    ]
})

task_id = response.json()["task_id"]

# Poll for results
while True:
    result = requests.get(f"http://localhost:8000/results/{task_id}")
    status = result.json()["status"]
    
    if status == "completed":
        final_data = result.json()["result_data"]
        print(f"Got {len(final_data)} selected posts!")
        break
    elif status == "error":
        print("Processing failed:", result.json()["error"])
        break
    
    time.sleep(5)  # Wait 5 seconds before checking again
```

## ğŸ¯ Pipeline Process

1. **URL Analysis**: Detect platforms and extract usernames
2. **Dynamic Scraper Update**: Update scraper files with provided URLs
3. **Parallel Scraping**: Execute platform-specific scrapers
4. **Data Processing**: Run `retrieve.py` to unify and standardize data
5. **Content Selection**: Run `results.py` to score and select top 9 posts
6. **Result Generation**: Return final `result.json` with post numbering

## ğŸ“Š Scoring Algorithm

### Platform-Specific Formulas

- **Twitter**: `Views Ã— 0.3 + (Likes Ã— 2 + Retweets Ã— 3 + Replies Ã— 1.5)`
- **LinkedIn**: `Comments Ã— 5 + Likes Ã— 1`
- **YouTube**: `Views Ã— 0.1`
- **TikTok**: `Views Ã— 0.2`
- **Instagram**: `Views Ã— 0.5`

### Content Distribution

- Target: 9 posts total
- Even distribution across unique URL groups
- Fallback to multiples of 3 (9 â†’ 6 â†’ 3) if insufficient content
- Highest scoring content prioritized within each group

## ğŸ”§ Configuration

### Scraper Timeouts
- Individual scrapers: 5 minutes
- Data processing: 2 minutes
- Content selection: 1 minute

### URL Limits
- Minimum: 1 URL
- Maximum: 10 URLs per request

### Supported Platforms
- âœ… Instagram (`instagram.com`)
- âœ… LinkedIn (`linkedin.com`)
- âœ… Twitter/X (`twitter.com`, `x.com`)
- âœ… YouTube (`youtube.com`, `youtu.be`)
- âœ… TikTok (`tiktok.com`)

## ğŸ“ File Structure

```
scrape/
â”œâ”€â”€ main.py              # FastAPI application
â”œâ”€â”€ retrieve.py          # Data processing
â”œâ”€â”€ results.py           # Content selection
â”œâ”€â”€ test_api.py          # API test script
â”œâ”€â”€ scrapers/
â”‚   â”œâ”€â”€ instagram.py     # Instagram scraper
â”‚   â”œâ”€â”€ linkedin.py      # LinkedIn scraper
â”‚   â”œâ”€â”€ twitter.py       # Twitter scraper
â”‚   â”œâ”€â”€ youtube.py       # YouTube scraper
â”‚   â””â”€â”€ tiktok.py        # TikTok scraper
â”œâ”€â”€ data.json           # Processed data (intermediate)
â””â”€â”€ result.json         # Final selected content
```

## ğŸ› Troubleshooting

### Common Issues

1. **"Task not found"**: The task_id has expired or doesn't exist
2. **"All scrapers failed"**: Check APIFY_KEY configuration
3. **Timeout errors**: Scrapers may take longer for accounts with lots of content
4. **Connection refused**: Make sure the server is running on port 8000

### Logs

Check the console output and `log.txt` file for detailed error messages.

### Debug Mode

Set logging level to DEBUG in `main.py`:
```python
logging.basicConfig(level=logging.DEBUG, ...)
```

## ğŸ”’ Security Notes

- Configure CORS properly for production
- Implement rate limiting
- Add authentication if needed
- Validate and sanitize URL inputs